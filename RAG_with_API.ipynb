{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sibikrish3000/chatbot_workshop/blob/main/RAG_with_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![3JB0VdF.md.png](https://iili.io/3JB0VdF.png)](https://freeimage.host/i/3JB0VdF)"
      ],
      "metadata": {
        "id": "2k6WRFykKV-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Documents\n",
        "question = \"i love coding\" # @param [\"What kinds of pets do I like?\"] {\"allow-input\":true}\n",
        "document = \"i love coding\"    # @param [\"My favorite pet is a cat.\"] {\"allow-input\":true}"
      ],
      "metadata": {
        "id": "74fYGlZ1K8ty"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indexing\n",
        "![](https://iili.io/3JBjI1f.png)"
      ],
      "metadata": {
        "id": "xjpEUdIoLmpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "9NYLGc1-L0f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What are tokens?\n",
        ">Tokens can be thought of as pieces of words. Before the API processes the request, the input is broken down into tokens. These tokens are not cut up exactly where the words start or end - tokens can include trailing spaces and even sub-words. Here are some helpful rules of thumb for understanding tokens in terms of lengths:\n",
        "\n",
        "1 token ~= 4 chars in English\n",
        "\n",
        "1 token ~= ¾ words\n",
        "\n",
        "100 tokens ~= 75 words\n",
        "\n",
        "Or\n",
        "\n",
        "1-2 sentence ~= 30 tokens\n",
        "\n",
        "1 paragraph ~= 100 tokens\n",
        "\n",
        "1,500 words ~= 2048 tokens"
      ],
      "metadata": {
        "id": "dICcW3tBP4Bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    enc_str=encoding.encode(string)\n",
        "    print(f'encoded string: {enc_str}')\n",
        "    dec_str=encoding.decode(enc_str)\n",
        "    print(f'Decode string: {dec_str}')\n",
        "    num_tokens = len(enc_str)\n",
        "    return num_tokens\n",
        "\n",
        "num_tokens_from_string(question, \"cl100k_base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz5L6YrNLJnf",
        "outputId": "bd1fb4c4-716f-4c11-d93e-14171adbf9ce"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded string: [72, 3021, 11058]\n",
            "Decode string: i love coding\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding\n",
        ">embedding is a representation of data (such as words, sentences, or other entities) in a continuous vector space. These vectors are typically dense, meaning they consist of real numbers rather than sparse binary values, and are designed to capture semantic relationships between the items being represented.\n",
        "\n",
        "Key Concepts of Embeddings:\n",
        "\n",
        "Vector Representation :\n",
        "\n",
        "Embeddings map discrete objects (e.g., words, images, or categories) into continuous vector spaces. For example, a word like \"king\" might be represented as a vector [0.25, -0.1, 0.9, ...] in a high-dimensional space.\n",
        "\n",
        "----\n",
        "\n",
        "Semantic Meaning :\n",
        "\n",
        "The key idea behind embeddings is that similar items will have similar vector representations. For instance, in word embeddings, semantically related words (like \"king\" and \"queen\") will have vectors that are close to each other in the vector space.\n",
        "\n",
        "* Suppose you have the words \"king\", \"queen\", \"man\", and \"woman\". After training, their embeddings might look something like this in a simplified 2D space:\n",
        "```\n",
        "king   -> [0.8, 0.2]\n",
        "queen  -> [0.7, 0.3]\n",
        "man    -> [0.9, 0.1]\n",
        "woman  -> [0.8, 0.4]\n",
        "```\n",
        "In this case, the vector for \"king\" is closer to \"man\" than to \"woman\", and \"queen\" is closer to \"woman\". Additionally, the relationship between \"king\" and \"queen\" might be similar to the relationship between \"man\" and \"woman\", which can be captured by vector arithmetic:\n",
        "```\n",
        "king - man + woman ≈ queen\n",
        "```"
      ],
      "metadata": {
        "id": "om-JWDfSMPuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "XhHC3Ub_NfuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "embd = HuggingFaceEmbeddings()\n",
        "query_result = embd.embed_query(question)\n",
        "document_result = embd.embed_query(document)\n",
        "print(f'query_result: {query_result}')\n",
        "print(f'query_result length: {len(query_result)}')\n",
        "print(f'document_result: {document_result}')\n",
        "print(f'document_result length: {len(document_result)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1gN9KhDMUy8",
        "outputId": "f3dd4baa-4e5a-45a3-80f1-73617041e260"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-275cde755a8d>:2: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  embd = HuggingFaceEmbeddings()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_result: [-0.029304862022399902, 0.06239829212427139, -0.064301036298275, 0.014795470051467419, 0.01879589818418026, 0.0608164444565773, -0.07478100061416626, -0.01659526117146015, -0.030508002266287804, 0.030598340556025505, 0.025639722123742104, 0.038709282875061035, 0.0024956215638667345, 0.1047094389796257, 0.009299385361373425, 0.01294324453920126, -0.022103862836956978, -0.022564884275197983, -0.03605072572827339, 0.00011076000373577699, 0.02002040110528469, 0.03963882476091385, -0.042874306440353394, -0.02453198842704296, -0.004480579402297735, -0.009060772135853767, 0.009875593706965446, -0.06069997325539589, -0.03463570773601532, 0.05754446983337402, -0.002160274889320135, -0.0021566080395132303, -0.005808633752167225, 0.0028201164677739143, 1.6196588603634154e-06, 0.004496601410210133, -0.015027355402708054, -0.002435229951515794, -0.05663324519991875, 0.02231852523982525, -0.030137047171592712, -0.021930424496531487, -0.02272447571158409, 0.007798372767865658, -0.0473465621471405, 0.023754384368658066, 0.0917426347732544, -0.05870368704199791, 0.029528871178627014, 0.00017409477732144296, -0.004552232567220926, -0.016986533999443054, -0.017124490812420845, -0.022883981466293335, 0.019126415252685547, 0.033929698169231415, 0.01704089716076851, 0.05406181514263153, 0.09688348323106766, 0.010747317224740982, -0.02364988997578621, -0.047129202634096146, 0.04012304171919823, 0.01001310721039772, 0.07217058539390564, 0.05043113976716995, 0.003910656087100506, -0.02521788515150547, 0.028067052364349365, 0.021775998175144196, 0.009601669386029243, -0.03881899267435074, -0.002839385299012065, 0.02536410093307495, 0.019633812829852104, -0.07761761546134949, -0.03287467733025551, 0.0472879596054554, 0.0025174091570079327, -0.01160526368767023, -0.006696982774883509, 0.008956176228821278, -0.008238988928496838, -0.02282298356294632, 0.009934370405972004, -0.0283819492906332, 0.0003002817975357175, -0.019606273621320724, -0.04657846316695213, 0.06873288750648499, 0.003942378330975771, -0.017649900168180466, 0.03996951878070831, 0.055884163826704025, -0.023013940081000328, 0.004712925758212805, -0.008467601612210274, 0.018591998144984245, 0.0064796824008226395, 0.024042172357439995, 0.002820840571075678, 0.045340172946453094, -0.007313026115298271, 0.02072312869131565, -0.019276544451713562, 0.01033987756818533, 0.01413769368082285, -0.002880614250898361, -0.0003788950853049755, 0.05597872659564018, -0.056972354650497437, -0.03653380274772644, 0.009467170573771, 0.028701959177851677, 0.010220987722277641, 0.08910344541072845, 0.01534245628863573, 0.06612782180309296, 0.007736260537058115, 0.01912735402584076, -0.017602108418941498, 0.004523506388068199, 0.013643178157508373, 0.059951893985271454, 0.04139655455946922, -0.062391169369220734, -0.04053959622979164, 0.004351409617811441, -0.043168794363737106, -0.010091309435665607, -0.03326224163174629, -0.012742179445922375, -0.0008257169974967837, -0.04352286085486412, -0.0007254367228597403, 0.02952035516500473, -0.06073034927248955, 0.02683776244521141, 0.04452318698167801, -0.03760207071900368, -0.007658577058464289, 0.003347768448293209, 0.02004939690232277, -0.03437725082039833, 0.02030937746167183, 0.008979144506156445, -0.007191873155534267, -0.0546085424721241, -0.00751173822209239, 0.02134014666080475, -0.004813270643353462, 0.06140127405524254, -0.10709904134273529, -0.004998167976737022, 0.03104027360677719, 0.01919749565422535, 0.005830214358866215, -0.040238965302705765, 0.001420048763975501, -0.0043757399544119835, 0.030351975932717323, -0.04722236469388008, 0.09378235042095184, 0.0004254042578395456, -0.03960655629634857, -0.019947201013565063, 0.006657011806964874, -0.011834236793220043, -0.013655927032232285, -0.052023954689502716, 0.03737131506204605, -0.0537535585463047, -0.038516815751791, 0.015297042205929756, 0.033193912357091904, -0.00042592332465574145, 0.04208427667617798, 0.032282229512929916, -0.02283627912402153, -0.05866088718175888, 0.005087024997919798, -0.07529543340206146, 0.04661894962191582, 0.061507102102041245, -0.010354074649512768, -0.004280518274754286, -0.05996645241975784, 0.004030450247228146, -0.015417995862662792, -0.019927505403757095, 0.0047317082062363625, 0.00470114778727293, 0.023009169846773148, 0.03317698836326599, 0.009003535844385624, -0.03488614410161972, 0.047599051147699356, -0.0508171021938324, 0.015590825118124485, -0.03355229273438454, -0.013625239953398705, -0.015268104150891304, -0.010445659048855305, 0.041406478732824326, -0.02980884723365307, -0.05800309404730797, -0.02479458414018154, -0.04805587977170944, 0.06632620841264725, -0.03278657793998718, 0.03565727919340134, -0.014457517303526402, 0.0047851987183094025, 0.006611213553696871, -0.003557969816029072, -0.05142012611031532, 0.021597841754555702, -0.02759735658764839, 0.006692448165267706, 0.033692240715026855, -0.03954809904098511, 0.030457137152552605, 0.025992434471845627, 0.02447989583015442, -0.0360005646944046, -0.04010183364152908, 0.011765958741307259, 0.05154206231236458, 0.006095946300774813, -0.0487666130065918, 0.005323311313986778, 0.016538601368665695, 0.02365649864077568, -0.0005270637338981032, 0.041761964559555054, -0.045548442751169205, 0.03023507632315159, -0.05000479519367218, -0.00048722862266004086, -0.031000040471553802, -0.01036432571709156, 0.02464001625776291, 0.013656907714903355, 0.057539138942956924, 0.06455837190151215, -0.03184911981225014, 0.02614973485469818, 0.02790270373225212, -0.0032649626955389977, -0.02879333682358265, -0.02552441507577896, 0.01483344566076994, -0.00018960345187224448, 0.0457904227077961, 0.04190267249941826, -0.035755742341279984, 0.006090101320296526, -0.013227425515651703, -0.023493800312280655, -0.044213488698005676, 0.03660955652594566, -0.06167403981089592, 0.013937936164438725, -0.0010326150804758072, 0.05112356320023537, -0.016887500882148743, 0.0034575629979372025, -0.02990601770579815, -0.0705595389008522, -0.008100619539618492, -0.0605337955057621, -0.009569800458848476, -0.029240349307656288, 0.017358670011162758, -0.004154668189585209, 0.012078647501766682, -0.024082209914922714, -0.04706864804029465, -0.026074351742863655, 0.013392605818808079, -0.026042025536298752, -0.01576646789908409, -0.005262935999780893, -0.013759488239884377, -0.00892641767859459, 0.009842167608439922, 0.048667941242456436, -0.03189360350370407, -0.010280810296535492, 0.027320951223373413, 0.014842788688838482, -0.033012889325618744, -0.0061251139268279076, -0.008735879324376583, -0.027350906282663345, -0.04811348393559456, -0.025131037458777428, -0.020877622067928314, -0.025357723236083984, -0.009832438081502914, 0.045946624130010605, -0.046662334352731705, 0.04938210919499397, 0.010192389599978924, 0.008634629659354687, -0.027182061225175858, 0.037929825484752655, -0.01390437688678503, -0.005754191894084215, 0.06251032650470734, 0.030254090204834938, 0.00089404103346169, -0.025998925790190697, -0.01628311723470688, 0.05147690325975418, 0.03412426635622978, -0.027396289631724358, -0.03387018293142319, 0.02224101684987545, 0.07014686614274979, -0.04109707847237587, -0.03567469120025635, 0.0360606387257576, 0.029423788189888, -0.035055480897426605, 0.017057234421372414, -0.08503030985593796, -0.015151944011449814, 0.03105708584189415, -0.051174428313970566, -0.013508000411093235, 0.02309354767203331, -0.006304576527327299, 0.009756367653608322, 0.0186349805444479, -0.025810446590185165, -0.03210680931806564, -0.010337363928556442, -0.052681125700473785, -0.025844896212220192, -0.001240914803929627, -0.039953283965587616, -0.046087756752967834, 0.01538085751235485, -0.037815988063812256, 0.023303480818867683, 0.017984746024012566, 0.010245749726891518, 0.03241367265582085, -0.04471221938729286, 0.05252876877784729, -0.03831509128212929, 0.026795051991939545, 0.013464308343827724, 0.01772226206958294, 0.01892373524606228, -0.014554182067513466, -0.008409060537815094, -0.08099616318941116, -0.022737249732017517, -0.035057224333286285, 0.016262443736195564, -0.0022878835443407297, 0.053597450256347656, -0.04696971923112869, 0.07587246596813202, 0.06650136411190033, 0.0294958408921957, -0.059841230511665344, 0.03201375529170036, -0.019434144720435143, 0.043793484568595886, -0.05440475791692734, 0.029098449274897575, 0.01919749192893505, -0.016329312697052956, 0.03189148008823395, 0.02045837603509426, -0.045912884175777435, 0.010740400291979313, 0.04079398512840271, -0.07678727060556412, -0.011764229275286198, 0.0037153398152440786, 0.02594490721821785, -0.03624160215258598, -0.0001852941932156682, -0.015782250091433525, -0.03644103184342384, 0.030658481642603874, 0.027561718598008156, -0.00798261258751154, -0.11677209287881851, -0.07838108390569687, -0.003323222277686, 0.013609013520181179, -0.08117423951625824, 0.035634372383356094, -0.014720420353114605, 0.039681389927864075, 0.0004851244739256799, 0.014301822520792484, 0.02402513474225998, 0.03738158941268921, 0.047779329121112823, 0.007399778347462416, 0.0804961770772934, 0.01766437105834484, -0.025654936209321022, -0.0004997979267500341, -0.007486558984965086, 0.016374830156564713, 0.022005511447787285, -0.023427944630384445, -0.0008658752776682377, 0.013448874466121197, -0.024523740634322166, 0.03100409545004368, 0.03380342945456505, -0.08701562881469727, -0.06404171884059906, 0.0033437313977628946, 0.07875268161296844, 0.01939483918249607, -0.005371297709643841, -0.011736988089978695, -0.0028776798862963915, 0.03404214605689049, -0.020984085276722908, -0.0017712299013510346, 0.005153275560587645, -0.007940567098557949, -0.042086318135261536, 0.04105747491121292, -0.04194536432623863, -0.06358816474676132, -0.08175133168697357, 0.04758119583129883, -0.04429902508854866, -0.04962947592139244, 0.009568961337208748, 0.08009093999862671, 0.009888343513011932, -0.037680692970752716, -0.046327754855155945, 0.031298715621232986, 0.02999052032828331, -0.11649967730045319, -0.09387196600437164, -0.02583497203886509, 0.0430203340947628, -0.03256640210747719, -0.036747489124536514, 0.01857583224773407, -0.022167066112160683, -0.02113906480371952, 0.034815188497304916, -0.0855879932641983, -0.021023059263825417, 0.00357545493170619, 0.038693442940711975, 0.027222348377108574, 0.01588776521384716, 0.0014050804311409593, -0.0057579511776566505, -0.09496753662824631, 0.029192345216870308, -0.05168696492910385, 0.05922204256057739, -0.0421244241297245, -0.0439467653632164, 0.011194020509719849, -0.06738012284040451, -0.03270426020026207, -0.03496700897812843, 0.07989226281642914, 0.008473283611238003, -0.027478091418743134, 0.009218242019414902, -0.02449929714202881, 0.013025593012571335, 0.006422656588256359, 0.02490304969251156, -0.018438661471009254, -0.008979163132607937, 0.014364359900355339, 0.01775355264544487, -0.02982785552740097, -0.01918136700987816, -0.0166019219905138, 0.02042986825108528, -0.020916905254125595, 0.008269120939075947, 0.005885526537895203, 0.013531609438359737, 0.011418682523071766, 0.019101034849882126, 0.007808696012943983, -0.021167941391468048, 0.06588874012231827, 0.025251328945159912, 0.0030808637384325266, -0.006742026191204786, 0.013751945458352566, 0.06873705238103867, -0.02480302006006241, -0.00039861450204625726, -0.012084854766726494, -0.05125662311911583, 0.06091511994600296, 0.00885255727916956, -0.011709053069353104, -0.01369162555783987, -0.09074164927005768, 0.00650453008711338, -0.01852564513683319, 0.02493143081665039, 0.02458404004573822, -0.03418659791350365, -0.012069675140082836, 0.02972322888672352, -0.017488114535808563, 0.04502009227871895, 0.0019106619292870164, -0.015934856608510017, -0.016649894416332245, 0.04499336704611778, -0.03363994136452675, -0.07565248012542725, 0.07677709311246872, 0.027235301211476326, -0.018004706129431725, -0.03168882429599762, -0.029420292004942894, -0.01904001273214817, 0.05954771488904953, 0.029031502082943916, 0.012872785329818726, -0.005862645339220762, 0.02094864286482334, 0.014992783777415752, -0.026760729029774666, 0.007458177395164967, 0.013883069157600403, 0.02613651752471924, 0.07557068765163422, 0.05685577169060707, 0.03493247181177139, -0.05375167354941368, 0.027821894735097885, -0.002621453022584319, 0.009951945394277573, 0.06372544914484024, -0.010834315791726112, -0.048843786120414734, -6.57585131503419e-33, -0.018727807328104973, -0.026378128677606583, -0.008825663477182388, 0.043211694806814194, -0.011037623509764671, -0.03182123228907585, 0.027261342853307724, 0.04719933122396469, 0.01568037085235119, -0.02643846534192562, -0.005570404697209597, 0.016527852043509483, 0.01572256349027157, -0.007679393980652094, 0.048344165086746216, -0.006221710704267025, 0.06671582907438278, 0.005410932470113039, -0.00046685055713169277, -0.008392672054469585, -0.032342784106731415, -0.025025127455592155, 0.010641810484230518, -0.003300674259662628, -0.004973028786480427, 0.07916028797626495, -0.014858231879770756, -0.009459530003368855, 0.042438361793756485, 0.00872894749045372, 0.012375426478683949, 0.023515619337558746, -0.023176664486527443, 0.08363069593906403, -0.004091983195394278, 0.022349881008267403, -0.022341646254062653, -0.045661117881536484, -0.03238058462738991, 0.0014136673416942358, -0.004945532884448767, -0.06800588965415955, 0.07647783309221268, -0.028833890333771706, 0.00165404356084764, -0.03413207828998566, 0.05528976023197174, -0.008957846090197563, 0.06273799389600754, -0.009352307766675949, -0.05560417100787163, 0.0013196691870689392, -0.06871584057807922, -0.025803783908486366, 0.007489404641091824, -0.09854598343372345, 0.054543282836675644, 0.0001455430028727278, -0.004838563967496157, 0.043878551572561264, -0.03404546156525612, 0.021690480411052704, -0.02626216597855091, -0.0029000043869018555, 0.006992335431277752, 0.009076282382011414, 0.07943841069936752, 0.003931026440113783, 0.014002359472215176, 0.08618728816509247, 0.011754738166928291, 0.03903926536440849, 0.010368292219936848, 0.03498253598809242, 0.03369910269975662, -0.05290665104985237, -0.023097112774848938, -0.0007343890611082315, -0.060828570276498795, -0.020987030118703842, 0.005217548925429583, 0.026725780218839645, -0.006606708280742168, -0.03753398731350899, 0.07492545247077942, 0.0024529777001589537, -0.0105771254748106, 0.0018746843561530113, -0.047276947647333145, 0.0293106772005558, 0.014977581799030304, 0.02413112297654152, -0.0206019077450037, -0.011674251407384872, 0.015910333022475243, 0.02948828414082527, -0.025318030267953873, 0.03691510111093521, -0.022213676944375038, -0.018493102863430977, 0.024656539782881737, 0.010742348618805408, -0.007684546522796154, -0.033385470509529114, 0.034495383501052856, 0.01653408072888851, -0.019618166610598564, 0.013713917694985867, 0.00025158014614135027, -0.016529645770788193, -0.01828000880777836, 0.010103225708007812, 0.06321084499359131, 0.028778506442904472, 0.04461602866649628, -0.0022679544053971767, 0.02823236584663391, -0.043433934450149536, -0.0580972321331501, -0.044721122831106186, -0.036401551216840744, -0.026860881596803665, -0.056323446333408356, 0.02298581227660179, 0.002940655453130603, -0.04837541654706001, -0.05172295123338699, 0.015475613996386528, 0.0015420104609802365, -0.01964982971549034, -0.024930516257882118, 0.009146804921329021, 2.2893020457104285e-07, 0.01050429604947567, 0.0445575937628746, -0.012117850594222546, 0.07725510001182556, 0.011911305598914623, 0.022133326157927513, -0.02440454065799713, 0.017086312174797058, 0.010007388889789581, 0.017389733344316483, -0.06254792213439941, 0.02040645107626915, 0.05484340339899063, -0.0018058388959616423, -0.0046567232348024845, 0.004003376234322786, 0.0443679615855217, 0.0017643673345446587, 0.007991486229002476, -0.008096118457615376, 0.10006806999444962, 0.02293204329907894, 0.06573460251092911, 0.013270551338791847, 0.02744591049849987, 0.010599168948829174, 0.01420672982931137, -0.042726900428533554, 0.11296803504228592, 0.03061257302761078, -0.1074194386601448, 0.03578902408480644, 0.00024448969634249806, 0.004007160197943449, -0.010912668891251087, -0.09249882400035858, 0.03865938261151314, 0.03210792317986488, 0.030704855918884277, 0.0801999494433403, -0.0349934846162796, 0.010253729298710823, -0.030437255278229713, -0.007403282914310694, -0.0034079025499522686, 0.011447424069046974, 0.03963613510131836, 0.023638607934117317, -0.00414345134049654, -0.020931370556354523, -0.026715870946645737, -0.01474079955369234, 0.017961623147130013, -0.020756276324391365, -0.002672639675438404, -0.022115537896752357, -0.023630734533071518, 0.02134062349796295, 0.005591946188360453, 0.011902746744453907, -0.04753683879971504, -0.00219096802175045, 0.013217215426266193, 0.038700152188539505, 0.08060214668512344, 0.020989399403333664, 0.006108321249485016, 1.5076311980020864e-34, 0.013684089295566082, -0.027184635400772095, -0.007269466761499643, 0.04452819377183914, 0.03245333209633827, -0.0010396792786195874, 0.010292352177202702, -0.013185882940888405, 0.005942948628216982, 0.03976302221417427, -0.04407054930925369]\n",
            "query_result length: 768\n",
            "document_result: [-0.029304862022399902, 0.06239829212427139, -0.064301036298275, 0.014795470051467419, 0.01879589818418026, 0.0608164444565773, -0.07478100061416626, -0.01659526117146015, -0.030508002266287804, 0.030598340556025505, 0.025639722123742104, 0.038709282875061035, 0.0024956215638667345, 0.1047094389796257, 0.009299385361373425, 0.01294324453920126, -0.022103862836956978, -0.022564884275197983, -0.03605072572827339, 0.00011076000373577699, 0.02002040110528469, 0.03963882476091385, -0.042874306440353394, -0.02453198842704296, -0.004480579402297735, -0.009060772135853767, 0.009875593706965446, -0.06069997325539589, -0.03463570773601532, 0.05754446983337402, -0.002160274889320135, -0.0021566080395132303, -0.005808633752167225, 0.0028201164677739143, 1.6196588603634154e-06, 0.004496601410210133, -0.015027355402708054, -0.002435229951515794, -0.05663324519991875, 0.02231852523982525, -0.030137047171592712, -0.021930424496531487, -0.02272447571158409, 0.007798372767865658, -0.0473465621471405, 0.023754384368658066, 0.0917426347732544, -0.05870368704199791, 0.029528871178627014, 0.00017409477732144296, -0.004552232567220926, -0.016986533999443054, -0.017124490812420845, -0.022883981466293335, 0.019126415252685547, 0.033929698169231415, 0.01704089716076851, 0.05406181514263153, 0.09688348323106766, 0.010747317224740982, -0.02364988997578621, -0.047129202634096146, 0.04012304171919823, 0.01001310721039772, 0.07217058539390564, 0.05043113976716995, 0.003910656087100506, -0.02521788515150547, 0.028067052364349365, 0.021775998175144196, 0.009601669386029243, -0.03881899267435074, -0.002839385299012065, 0.02536410093307495, 0.019633812829852104, -0.07761761546134949, -0.03287467733025551, 0.0472879596054554, 0.0025174091570079327, -0.01160526368767023, -0.006696982774883509, 0.008956176228821278, -0.008238988928496838, -0.02282298356294632, 0.009934370405972004, -0.0283819492906332, 0.0003002817975357175, -0.019606273621320724, -0.04657846316695213, 0.06873288750648499, 0.003942378330975771, -0.017649900168180466, 0.03996951878070831, 0.055884163826704025, -0.023013940081000328, 0.004712925758212805, -0.008467601612210274, 0.018591998144984245, 0.0064796824008226395, 0.024042172357439995, 0.002820840571075678, 0.045340172946453094, -0.007313026115298271, 0.02072312869131565, -0.019276544451713562, 0.01033987756818533, 0.01413769368082285, -0.002880614250898361, -0.0003788950853049755, 0.05597872659564018, -0.056972354650497437, -0.03653380274772644, 0.009467170573771, 0.028701959177851677, 0.010220987722277641, 0.08910344541072845, 0.01534245628863573, 0.06612782180309296, 0.007736260537058115, 0.01912735402584076, -0.017602108418941498, 0.004523506388068199, 0.013643178157508373, 0.059951893985271454, 0.04139655455946922, -0.062391169369220734, -0.04053959622979164, 0.004351409617811441, -0.043168794363737106, -0.010091309435665607, -0.03326224163174629, -0.012742179445922375, -0.0008257169974967837, -0.04352286085486412, -0.0007254367228597403, 0.02952035516500473, -0.06073034927248955, 0.02683776244521141, 0.04452318698167801, -0.03760207071900368, -0.007658577058464289, 0.003347768448293209, 0.02004939690232277, -0.03437725082039833, 0.02030937746167183, 0.008979144506156445, -0.007191873155534267, -0.0546085424721241, -0.00751173822209239, 0.02134014666080475, -0.004813270643353462, 0.06140127405524254, -0.10709904134273529, -0.004998167976737022, 0.03104027360677719, 0.01919749565422535, 0.005830214358866215, -0.040238965302705765, 0.001420048763975501, -0.0043757399544119835, 0.030351975932717323, -0.04722236469388008, 0.09378235042095184, 0.0004254042578395456, -0.03960655629634857, -0.019947201013565063, 0.006657011806964874, -0.011834236793220043, -0.013655927032232285, -0.052023954689502716, 0.03737131506204605, -0.0537535585463047, -0.038516815751791, 0.015297042205929756, 0.033193912357091904, -0.00042592332465574145, 0.04208427667617798, 0.032282229512929916, -0.02283627912402153, -0.05866088718175888, 0.005087024997919798, -0.07529543340206146, 0.04661894962191582, 0.061507102102041245, -0.010354074649512768, -0.004280518274754286, -0.05996645241975784, 0.004030450247228146, -0.015417995862662792, -0.019927505403757095, 0.0047317082062363625, 0.00470114778727293, 0.023009169846773148, 0.03317698836326599, 0.009003535844385624, -0.03488614410161972, 0.047599051147699356, -0.0508171021938324, 0.015590825118124485, -0.03355229273438454, -0.013625239953398705, -0.015268104150891304, -0.010445659048855305, 0.041406478732824326, -0.02980884723365307, -0.05800309404730797, -0.02479458414018154, -0.04805587977170944, 0.06632620841264725, -0.03278657793998718, 0.03565727919340134, -0.014457517303526402, 0.0047851987183094025, 0.006611213553696871, -0.003557969816029072, -0.05142012611031532, 0.021597841754555702, -0.02759735658764839, 0.006692448165267706, 0.033692240715026855, -0.03954809904098511, 0.030457137152552605, 0.025992434471845627, 0.02447989583015442, -0.0360005646944046, -0.04010183364152908, 0.011765958741307259, 0.05154206231236458, 0.006095946300774813, -0.0487666130065918, 0.005323311313986778, 0.016538601368665695, 0.02365649864077568, -0.0005270637338981032, 0.041761964559555054, -0.045548442751169205, 0.03023507632315159, -0.05000479519367218, -0.00048722862266004086, -0.031000040471553802, -0.01036432571709156, 0.02464001625776291, 0.013656907714903355, 0.057539138942956924, 0.06455837190151215, -0.03184911981225014, 0.02614973485469818, 0.02790270373225212, -0.0032649626955389977, -0.02879333682358265, -0.02552441507577896, 0.01483344566076994, -0.00018960345187224448, 0.0457904227077961, 0.04190267249941826, -0.035755742341279984, 0.006090101320296526, -0.013227425515651703, -0.023493800312280655, -0.044213488698005676, 0.03660955652594566, -0.06167403981089592, 0.013937936164438725, -0.0010326150804758072, 0.05112356320023537, -0.016887500882148743, 0.0034575629979372025, -0.02990601770579815, -0.0705595389008522, -0.008100619539618492, -0.0605337955057621, -0.009569800458848476, -0.029240349307656288, 0.017358670011162758, -0.004154668189585209, 0.012078647501766682, -0.024082209914922714, -0.04706864804029465, -0.026074351742863655, 0.013392605818808079, -0.026042025536298752, -0.01576646789908409, -0.005262935999780893, -0.013759488239884377, -0.00892641767859459, 0.009842167608439922, 0.048667941242456436, -0.03189360350370407, -0.010280810296535492, 0.027320951223373413, 0.014842788688838482, -0.033012889325618744, -0.0061251139268279076, -0.008735879324376583, -0.027350906282663345, -0.04811348393559456, -0.025131037458777428, -0.020877622067928314, -0.025357723236083984, -0.009832438081502914, 0.045946624130010605, -0.046662334352731705, 0.04938210919499397, 0.010192389599978924, 0.008634629659354687, -0.027182061225175858, 0.037929825484752655, -0.01390437688678503, -0.005754191894084215, 0.06251032650470734, 0.030254090204834938, 0.00089404103346169, -0.025998925790190697, -0.01628311723470688, 0.05147690325975418, 0.03412426635622978, -0.027396289631724358, -0.03387018293142319, 0.02224101684987545, 0.07014686614274979, -0.04109707847237587, -0.03567469120025635, 0.0360606387257576, 0.029423788189888, -0.035055480897426605, 0.017057234421372414, -0.08503030985593796, -0.015151944011449814, 0.03105708584189415, -0.051174428313970566, -0.013508000411093235, 0.02309354767203331, -0.006304576527327299, 0.009756367653608322, 0.0186349805444479, -0.025810446590185165, -0.03210680931806564, -0.010337363928556442, -0.052681125700473785, -0.025844896212220192, -0.001240914803929627, -0.039953283965587616, -0.046087756752967834, 0.01538085751235485, -0.037815988063812256, 0.023303480818867683, 0.017984746024012566, 0.010245749726891518, 0.03241367265582085, -0.04471221938729286, 0.05252876877784729, -0.03831509128212929, 0.026795051991939545, 0.013464308343827724, 0.01772226206958294, 0.01892373524606228, -0.014554182067513466, -0.008409060537815094, -0.08099616318941116, -0.022737249732017517, -0.035057224333286285, 0.016262443736195564, -0.0022878835443407297, 0.053597450256347656, -0.04696971923112869, 0.07587246596813202, 0.06650136411190033, 0.0294958408921957, -0.059841230511665344, 0.03201375529170036, -0.019434144720435143, 0.043793484568595886, -0.05440475791692734, 0.029098449274897575, 0.01919749192893505, -0.016329312697052956, 0.03189148008823395, 0.02045837603509426, -0.045912884175777435, 0.010740400291979313, 0.04079398512840271, -0.07678727060556412, -0.011764229275286198, 0.0037153398152440786, 0.02594490721821785, -0.03624160215258598, -0.0001852941932156682, -0.015782250091433525, -0.03644103184342384, 0.030658481642603874, 0.027561718598008156, -0.00798261258751154, -0.11677209287881851, -0.07838108390569687, -0.003323222277686, 0.013609013520181179, -0.08117423951625824, 0.035634372383356094, -0.014720420353114605, 0.039681389927864075, 0.0004851244739256799, 0.014301822520792484, 0.02402513474225998, 0.03738158941268921, 0.047779329121112823, 0.007399778347462416, 0.0804961770772934, 0.01766437105834484, -0.025654936209321022, -0.0004997979267500341, -0.007486558984965086, 0.016374830156564713, 0.022005511447787285, -0.023427944630384445, -0.0008658752776682377, 0.013448874466121197, -0.024523740634322166, 0.03100409545004368, 0.03380342945456505, -0.08701562881469727, -0.06404171884059906, 0.0033437313977628946, 0.07875268161296844, 0.01939483918249607, -0.005371297709643841, -0.011736988089978695, -0.0028776798862963915, 0.03404214605689049, -0.020984085276722908, -0.0017712299013510346, 0.005153275560587645, -0.007940567098557949, -0.042086318135261536, 0.04105747491121292, -0.04194536432623863, -0.06358816474676132, -0.08175133168697357, 0.04758119583129883, -0.04429902508854866, -0.04962947592139244, 0.009568961337208748, 0.08009093999862671, 0.009888343513011932, -0.037680692970752716, -0.046327754855155945, 0.031298715621232986, 0.02999052032828331, -0.11649967730045319, -0.09387196600437164, -0.02583497203886509, 0.0430203340947628, -0.03256640210747719, -0.036747489124536514, 0.01857583224773407, -0.022167066112160683, -0.02113906480371952, 0.034815188497304916, -0.0855879932641983, -0.021023059263825417, 0.00357545493170619, 0.038693442940711975, 0.027222348377108574, 0.01588776521384716, 0.0014050804311409593, -0.0057579511776566505, -0.09496753662824631, 0.029192345216870308, -0.05168696492910385, 0.05922204256057739, -0.0421244241297245, -0.0439467653632164, 0.011194020509719849, -0.06738012284040451, -0.03270426020026207, -0.03496700897812843, 0.07989226281642914, 0.008473283611238003, -0.027478091418743134, 0.009218242019414902, -0.02449929714202881, 0.013025593012571335, 0.006422656588256359, 0.02490304969251156, -0.018438661471009254, -0.008979163132607937, 0.014364359900355339, 0.01775355264544487, -0.02982785552740097, -0.01918136700987816, -0.0166019219905138, 0.02042986825108528, -0.020916905254125595, 0.008269120939075947, 0.005885526537895203, 0.013531609438359737, 0.011418682523071766, 0.019101034849882126, 0.007808696012943983, -0.021167941391468048, 0.06588874012231827, 0.025251328945159912, 0.0030808637384325266, -0.006742026191204786, 0.013751945458352566, 0.06873705238103867, -0.02480302006006241, -0.00039861450204625726, -0.012084854766726494, -0.05125662311911583, 0.06091511994600296, 0.00885255727916956, -0.011709053069353104, -0.01369162555783987, -0.09074164927005768, 0.00650453008711338, -0.01852564513683319, 0.02493143081665039, 0.02458404004573822, -0.03418659791350365, -0.012069675140082836, 0.02972322888672352, -0.017488114535808563, 0.04502009227871895, 0.0019106619292870164, -0.015934856608510017, -0.016649894416332245, 0.04499336704611778, -0.03363994136452675, -0.07565248012542725, 0.07677709311246872, 0.027235301211476326, -0.018004706129431725, -0.03168882429599762, -0.029420292004942894, -0.01904001273214817, 0.05954771488904953, 0.029031502082943916, 0.012872785329818726, -0.005862645339220762, 0.02094864286482334, 0.014992783777415752, -0.026760729029774666, 0.007458177395164967, 0.013883069157600403, 0.02613651752471924, 0.07557068765163422, 0.05685577169060707, 0.03493247181177139, -0.05375167354941368, 0.027821894735097885, -0.002621453022584319, 0.009951945394277573, 0.06372544914484024, -0.010834315791726112, -0.048843786120414734, -6.57585131503419e-33, -0.018727807328104973, -0.026378128677606583, -0.008825663477182388, 0.043211694806814194, -0.011037623509764671, -0.03182123228907585, 0.027261342853307724, 0.04719933122396469, 0.01568037085235119, -0.02643846534192562, -0.005570404697209597, 0.016527852043509483, 0.01572256349027157, -0.007679393980652094, 0.048344165086746216, -0.006221710704267025, 0.06671582907438278, 0.005410932470113039, -0.00046685055713169277, -0.008392672054469585, -0.032342784106731415, -0.025025127455592155, 0.010641810484230518, -0.003300674259662628, -0.004973028786480427, 0.07916028797626495, -0.014858231879770756, -0.009459530003368855, 0.042438361793756485, 0.00872894749045372, 0.012375426478683949, 0.023515619337558746, -0.023176664486527443, 0.08363069593906403, -0.004091983195394278, 0.022349881008267403, -0.022341646254062653, -0.045661117881536484, -0.03238058462738991, 0.0014136673416942358, -0.004945532884448767, -0.06800588965415955, 0.07647783309221268, -0.028833890333771706, 0.00165404356084764, -0.03413207828998566, 0.05528976023197174, -0.008957846090197563, 0.06273799389600754, -0.009352307766675949, -0.05560417100787163, 0.0013196691870689392, -0.06871584057807922, -0.025803783908486366, 0.007489404641091824, -0.09854598343372345, 0.054543282836675644, 0.0001455430028727278, -0.004838563967496157, 0.043878551572561264, -0.03404546156525612, 0.021690480411052704, -0.02626216597855091, -0.0029000043869018555, 0.006992335431277752, 0.009076282382011414, 0.07943841069936752, 0.003931026440113783, 0.014002359472215176, 0.08618728816509247, 0.011754738166928291, 0.03903926536440849, 0.010368292219936848, 0.03498253598809242, 0.03369910269975662, -0.05290665104985237, -0.023097112774848938, -0.0007343890611082315, -0.060828570276498795, -0.020987030118703842, 0.005217548925429583, 0.026725780218839645, -0.006606708280742168, -0.03753398731350899, 0.07492545247077942, 0.0024529777001589537, -0.0105771254748106, 0.0018746843561530113, -0.047276947647333145, 0.0293106772005558, 0.014977581799030304, 0.02413112297654152, -0.0206019077450037, -0.011674251407384872, 0.015910333022475243, 0.02948828414082527, -0.025318030267953873, 0.03691510111093521, -0.022213676944375038, -0.018493102863430977, 0.024656539782881737, 0.010742348618805408, -0.007684546522796154, -0.033385470509529114, 0.034495383501052856, 0.01653408072888851, -0.019618166610598564, 0.013713917694985867, 0.00025158014614135027, -0.016529645770788193, -0.01828000880777836, 0.010103225708007812, 0.06321084499359131, 0.028778506442904472, 0.04461602866649628, -0.0022679544053971767, 0.02823236584663391, -0.043433934450149536, -0.0580972321331501, -0.044721122831106186, -0.036401551216840744, -0.026860881596803665, -0.056323446333408356, 0.02298581227660179, 0.002940655453130603, -0.04837541654706001, -0.05172295123338699, 0.015475613996386528, 0.0015420104609802365, -0.01964982971549034, -0.024930516257882118, 0.009146804921329021, 2.2893020457104285e-07, 0.01050429604947567, 0.0445575937628746, -0.012117850594222546, 0.07725510001182556, 0.011911305598914623, 0.022133326157927513, -0.02440454065799713, 0.017086312174797058, 0.010007388889789581, 0.017389733344316483, -0.06254792213439941, 0.02040645107626915, 0.05484340339899063, -0.0018058388959616423, -0.0046567232348024845, 0.004003376234322786, 0.0443679615855217, 0.0017643673345446587, 0.007991486229002476, -0.008096118457615376, 0.10006806999444962, 0.02293204329907894, 0.06573460251092911, 0.013270551338791847, 0.02744591049849987, 0.010599168948829174, 0.01420672982931137, -0.042726900428533554, 0.11296803504228592, 0.03061257302761078, -0.1074194386601448, 0.03578902408480644, 0.00024448969634249806, 0.004007160197943449, -0.010912668891251087, -0.09249882400035858, 0.03865938261151314, 0.03210792317986488, 0.030704855918884277, 0.0801999494433403, -0.0349934846162796, 0.010253729298710823, -0.030437255278229713, -0.007403282914310694, -0.0034079025499522686, 0.011447424069046974, 0.03963613510131836, 0.023638607934117317, -0.00414345134049654, -0.020931370556354523, -0.026715870946645737, -0.01474079955369234, 0.017961623147130013, -0.020756276324391365, -0.002672639675438404, -0.022115537896752357, -0.023630734533071518, 0.02134062349796295, 0.005591946188360453, 0.011902746744453907, -0.04753683879971504, -0.00219096802175045, 0.013217215426266193, 0.038700152188539505, 0.08060214668512344, 0.020989399403333664, 0.006108321249485016, 1.5076311980020864e-34, 0.013684089295566082, -0.027184635400772095, -0.007269466761499643, 0.04452819377183914, 0.03245333209633827, -0.0010396792786195874, 0.010292352177202702, -0.013185882940888405, 0.005942948628216982, 0.03976302221417427, -0.04407054930925369]\n",
            "document_result length: 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cosine similarity"
      ],
      "metadata": {
        "id": "_bMRjEZKOoQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "similarity = cosine_similarity(query_result, document_result)\n",
        "print(\"Cosine Similarity:\", similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtDxn90UOiKC",
        "outputId": "df08c378-a4fb-43f7-8e9a-62e44570e159"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loader"
      ],
      "metadata": {
        "id": "tvXOMrAnRk-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### INDEXING ####\n",
        "\n",
        "# Loading blog\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "blog_docs = loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkamzqkaRoB-",
        "outputId": "b984dce5-14f8-4477-b2e9-c8edceeaec1f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blog_docs"
      ],
      "metadata": {
        "id": "-vpND8lFRwyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [**Text Splitter**](https://python.langchain.com/docs/how_to/recursive_text_splitter/)\n",
        "\n",
        "\n",
        "\n",
        "> This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f_KKFRMqSUZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50)\n",
        "\n",
        "# Make splits\n",
        "splits = text_splitter.split_documents(blog_docs)"
      ],
      "metadata": {
        "id": "dzYLo5lZTF8Y"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaTBNBoKTHog",
        "outputId": "b8ec7348-16b9-4652-a1f9-339eaf54c596"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorstore"
      ],
      "metadata": {
        "id": "cyZsjJZTTzeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "02kcJJg_UHm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "vectorstore = Chroma.from_documents(documents=splits,\n",
        "                                    embedding=HuggingFaceEmbeddings())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFUXT1DIT_E5",
        "outputId": "a0bbf2ce-c6a0-4f15-ee2a-d4b66b9d35b1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-dd0ac9e1ebd2>:3: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  embedding=HuggingFaceEmbeddings())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
      ],
      "metadata": {
        "id": "-BbrPqEzU5S5"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")"
      ],
      "metadata": {
        "id": "mF2mOkxOUtjn"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9TV7f_CVXUc",
        "outputId": "0fab5088-fab1-4ea5-a9e9-e8289a9bc60b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKFTF9dIUu1Y",
        "outputId": "72c38332-1125-47e9-8ad3-c580ca6be63d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:')"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3THSM2k2VKgV",
        "outputId": "d2e73da5-db8f-4c38-f030-04de1bbf0183"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 4: Generation\n",
        "![](https://iili.io/3JCzBQn.png)"
      ],
      "metadata": {
        "id": "VzpnIV5fVuuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Prompt\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCeK_r3qV1RC",
        "outputId": "d5857d00-1f39-418b-ca83-2981760f845e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-huggingface"
      ],
      "metadata": {
        "id": "-o77zrMeXG0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "# LLM\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"deepseek-ai/DeepSeek-R1\",\n",
        "     temperature=0.2 #@param {\"type\":\"number\",\"placeholder\":\"0.7\"}\n",
        "    ,\n",
        "    max_length=512,\n",
        "    task='text-generation',\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBOMO8D5Wdlw",
        "outputId": "490836dc-316b-4439-b4a3-6ef5e55ac386"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_huggingface.llms.huggingface_endpoint:WARNING! max_length is not default parameter.\n",
            "                    max_length was transferred to model_kwargs.\n",
            "                    Please make sure that max_length is what you intended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain\n",
        "\n",
        "chain = prompt | llm #chain pipeline\n",
        "\n",
        "# ### without pipeline\n",
        "# input=prompt.format_prompt(context=docs[0].page_content, question=\"What is Task Decomposition?\")\n",
        "# response=llm.invoke(input)\n",
        "# response"
      ],
      "metadata": {
        "id": "uWoIGcVhXxGd"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "chain.invoke({\"context\":docs[1],\"question\":\"What is Task Decomposition?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "uirDvx7WZ4gd",
        "outputId": "278815c0-313d-43b5-fc38-d336ec38e1db"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer: Task Decomposition is the process of breaking down a complex task into smaller, more manageable steps or subgoals. This can be done by a Large Language Model (LLM) using simple prompts, task-specific instructions, or with human inputs.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\") #@markdown create a langsmith acount here [**smith.langchain.com**](https://smith.langchain.com/) then generate langsmith API key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGABtIoNankX",
        "outputId": "dcae5cc0-c14c-4850-dadf-cc0d99ad278c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_hub_rag"
      ],
      "metadata": {
        "id": "du0xRVHIb7c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [**RAG chains**]()"
      ],
      "metadata": {
        "id": "JpD3Wq84cAry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Add document formatting step\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": retriever | (lambda docs: \"\\n\\n\".join(d.page_content for d in docs)),\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "qiAcXfr0clNf",
        "outputId": "5870300d-64fd-475c-f025-cc6a5731ff7e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer: Task Decomposition is the process of breaking down a complex task into smaller, more manageable subtasks. This can be done by the LLM with simple prompting, using task-specific instructions, or with human inputs. The goal is to transform big tasks into multiple manageable tasks to make them easier to understand and execute.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaimoKrU4kG7"
      },
      "source": [
        "# Installing Required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjhXLrM5D2tP",
        "outputId": "7bf8db4e-0773-409a-ce14-6cdc58b4915d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.3.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting langchain-experimental\n",
            "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.6-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.2.4-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.37)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.61.1)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
            "  Downloading groq-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.3.0-py3-none-any.whl (300 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.7/300.7 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.6-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_groq-0.2.4-py3-none-any.whl (14 kB)\n",
            "Downloading langchain_community-0.3.18-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading groq-0.18.0-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.0-py3-none-any.whl (30 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, pypdf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, typing-inspect, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pydantic-settings, nvidia-cusolver-cu12, groq, dataclasses-json, langchain-openai, langchain-groq, langchain-community, langchain-experimental\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.10.0 groq-0.18.0 httpx-sse-0.4.0 langchain-community-0.3.18 langchain-experimental-0.3.4 langchain-groq-0.2.4 langchain-openai-0.3.6 marshmallow-3.26.1 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydantic-settings-2.8.0 pypdf-5.3.0 python-dotenv-1.0.1 tiktoken-0.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "#@title <b>Installing Required packages</b>\n",
        "!pip install langchain faiss-cpu sentence-transformers transformers accelerate pypdf langchain-experimental langchain-groq langchain-community unstructured[all]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yObw6VkH4qaX"
      },
      "source": [
        "#RAG with API's"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB26SnEAEcgP"
      },
      "source": [
        "### Groq API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "J3O3n6ceER9o"
      },
      "outputs": [],
      "source": [
        "#@title <b>RAG implementaion with Groq API </b>\n",
        "import dotenv\n",
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "groq_api_key = userdata.get('GROQ_API_KEY') # @param [\"userdata.get('GROQ_API_KEY')\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "\n",
        "# ## load the Groq API key\n",
        "# dotenv.load_dotenv()\n",
        "# groq_api_key=os.environ['GROQ_API_KEY']\n",
        "\n",
        "model_name = \"llama-3.2-3b-preview\" # @param [\"llama-3.2-3b-preview\",\"deepseek-r1-distill-llama-70b\",\"llama-3.3-70b-versatile\",\"llama-3.3-70b-specdec\",\"llama-3.2-1b-preview\",\"llama-3.1-8b-instant\",\"llama3-70b-8192\",\"mixtral-8x7b-32768\",\"llama3-8b-8192\",\"llama-guard-3-8b\"]\n",
        "\n",
        "llm=ChatGroq(groq_api_key=groq_api_key,\n",
        "             model_name=model_name,\n",
        "temperature = 0.7 # @param {\"type\":\"number\",\"placeholder\":\"0.7\"}\n",
        "             )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEOheMm8Em7v"
      },
      "source": [
        "### Mistralai API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oOYv6wOofNTw"
      },
      "outputs": [],
      "source": [
        "#@title <b>RAG implementaion with Mistral API\n",
        "from google.colab import userdata\n",
        "\n",
        "mistral_api_key = userdata.get('MISTRAL_API_KEY') # @param [\"userdata.get('MISTRAL_API_KEY')\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "\n",
        "model_name = \"mistral-small-latest\" # @param [\"mistral-small-latest\",\"pixtral-large-latest\",\"codestral-latest\",\"mistral-large-latest\",\"ministral-8b-latest\",\"ministral-3b-latest\",\"open-mistral-nemo\"]\n",
        "\n",
        "llm = ChatMistralAI(\n",
        "    mistral_api_key=mistral_api_key,\n",
        "    model=model_name,\n",
        "    temperature = 0.7 # @param {\"type\":\"number\",\"placeholder\":\"0.7\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iE8EvQvaOOT5"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-mistralai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ObpLsQnE5C0"
      },
      "source": [
        "### Google GenerativeAI API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ngc4-nXhjZhX"
      },
      "outputs": [],
      "source": [
        "#@title <b>RAG implementaion with Google GenerativeAI API\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# 1. Configure Google AI Studio get from https://makersuite.google.com/\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY') # @param [\"userdata.get('GOOGLE_API_KEY')\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "\n",
        "model_name = \"gemini-2.0-flash\" # @param [\"gemini-2.0-flash\",\"gemini-2.0-flash-lite-preview-02-05\",\"gemini-2.0-pro-exp-02-05\",\"gemini-2.0-flash-thinking-exp-01-21\",\"gemini-2.0-flash-exp\",\"learnlm-1.5-pro-experimental\",\"gemini-1.5-pro\",\"gemini-1.5-flash\",\"gemini-1.5-8b\",\"gemini-1.5-flash-8b\"]\n",
        "\n",
        "# 2. Initialize Gemini model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY,\n",
        "    temperature = 0.7 # @param {\"type\":\"number\",\"placeholder\":\"0.7\"}\n",
        "    ,\n",
        "    convert_system_message_to_human=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l44-V9pdOWN3"
      },
      "outputs": [],
      "source": [
        "!pip install google-generativeai langchain-google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Running RAG with Locally with Ollama**\n"
      ],
      "metadata": {
        "id": "KAOyLHy70w2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Downloading Ollama**\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "_cPg1-hi1FNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash --bg\n",
        "#@title Starting ollama server in colab background\n",
        "ollama serve > /dev/null 2>&1  # Start server in background, suppress logs"
      ],
      "metadata": {
        "id": "pTeac1ZA1lK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl http://localhost:11434  # Should return \"Ollama is running\""
      ],
      "metadata": {
        "id": "wDrKy5G93XCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Downloading Deepseek-r1:7.5b Locally\n",
        "!ollama pull deepseek-r1"
      ],
      "metadata": {
        "id": "6W2T70252sA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#List the local Ollama models\n",
        "!ollama list"
      ],
      "metadata": {
        "id": "JVL7z5723AkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **kill Running Ollama**\n",
        "!pkill -f \"ollama serve\" #Dont run this cell unless u want to stop ollama server"
      ],
      "metadata": {
        "id": "N0ZyBvED4EyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Testing the Deepseek-r1  model in locally\n",
        "!ollama run --verbose deepseek-r1 'hi'"
      ],
      "metadata": {
        "id": "ENjMeIto3nih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U langchain-ollama"
      ],
      "metadata": {
        "id": "CyXg_YJF6eC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Initialize the Ollama model with Python Langchain**\n",
        "\n",
        "from langchain_community.llms import Ollama\n",
        "# from langchain_ollama.llms import OllamaLLM\n",
        "\n",
        "# Initialize the Ollama model\n",
        "llm = Ollama(\n",
        "    model=\"deepseek-r1\",\n",
        "    temperature=0.9,\n",
        "    base_url='http://localhost:11434'\n",
        ")\n",
        "\n",
        "# Run inference\n",
        "response = llm.invoke(\"Explain quantum computing in 3 sentences.\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "RkARqFwP5-Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZazokNW9FNI-"
      },
      "source": [
        "### Testing LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX3SQP78En4T",
        "outputId": "f0379f83-88c2-4b09-a03a-c47830f05ead"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Medicaid managed care is a system where a state contracts with managed care organizations (MCOs) to administer healthcare services to Medicaid beneficiaries. Instead of the state directly paying providers for each service (fee-for-service), the state pays the MCO a fixed per-member, per-month (capitation) rate to cover the cost of healthcare for the enrollees in that plan.\\n\\nHere's a breakdown of key aspects:\\n\\n*   **How it works:** States contract with MCOs (which can be HMOs, provider-sponsored organizations, or other types of health plans) to provide a defined set of healthcare services to Medicaid enrollees. Enrollees typically choose a managed care plan from a selection offered in their area.\\n\\n*   **Capitation:** The MCO receives a fixed payment per member per month, regardless of how much or how little healthcare the enrollee uses. This incentivizes the MCO to manage costs and promote preventive care.\\n\\n*   **Network of Providers:** MCOs create and manage a network of doctors, hospitals, and other healthcare providers that enrollees can access. Enrollees typically need to use providers within the plan's network to have their care covered.\\n\\n*   **Goals:** States use managed care to:\\n    *   Control Medicaid costs\\n    *   Improve access to care\\n    *   Enhance the quality of care\\n    *   Promote care coordination\\n\\n*   **Types of Managed Care:** There are different models, including:\\n    *   **Comprehensive Risk-Based Managed Care:** MCOs assume full financial risk for a comprehensive set of services.\\n    *   **Primary Care Case Management (PCCM):** States contract with primary care providers to manage the care of Medicaid enrollees.\\n    *   **Limited Benefit Plans:** MCOs provide a limited set of services, such as behavioral health or dental care.\\n\\n*   **Enrollment:** In many states, Medicaid beneficiaries are required to enroll in a managed care plan. Some populations, such as individuals with disabilities or those requiring long-term care, may be excluded or have specialized managed care options.\\n\\n*   **Oversight:** States and the federal government (Centers for Medicare & Medicaid Services - CMS) oversee Medicaid managed care plans to ensure they meet quality standards, provide adequate access to care, and comply with regulations.\\n\\nIn summary, Medicaid managed care is a significant way that states deliver healthcare services to Medicaid beneficiaries, aiming to improve efficiency, access, and quality of care through contracts with managed care organizations.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-82bbeee1-b1f2-4b46-9b2e-e96fee2acc02-0', usage_metadata={'input_tokens': 24, 'output_tokens': 526, 'total_tokens': 550, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.schema.messages import HumanMessage, SystemMessage\n",
        "messages = [\n",
        " SystemMessage(\n",
        "      content=\"\"\"You're an assistant knowledgeable about\n",
        "     healthcare. Only answer healthcare-related questions.\"\"\"\n",
        "),\n",
        " HumanMessage(content=\"What is Medicaid managed care?\"),\n",
        "]\n",
        "llm.invoke(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q5cCtLqFvUw"
      },
      "source": [
        "## Building a RAG Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGZFw9plFUqj"
      },
      "outputs": [],
      "source": [
        "#@title <b>Load documents with multiple file support</b>\n",
        "from langchain.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader, UnstructuredMarkdownLoader\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "import os\n",
        "# 1. Load documents with multiple file support\n",
        "def load_documents(path):\n",
        "    loaders = [\n",
        "        DirectoryLoader(path, glob=\"**/*.pdf\", loader_cls=PyPDFLoader),\n",
        "        DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader),\n",
        "        DirectoryLoader(path, glob=\"**/*.md\", loader_cls=UnstructuredMarkdownLoader)\n",
        "    ]\n",
        "    documents = []\n",
        "    for loader in loaders:\n",
        "        documents.extend(loader.load())\n",
        "    return documents\n",
        "\n",
        "knowledge_base_path = \"knowledge_base/\" # @param [\"knowledge_base/\"] {\"allow-input\":true}\n",
        "docs = load_documents(knowledge_base_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFgmxZOfGrQK"
      },
      "outputs": [],
      "source": [
        "#@title <b>Splitting text into semantic chunks</b>\n",
        "text_splitter = SemanticChunker(HuggingFaceEmbeddings())\n",
        "documents = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0RKCM73L98U"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1ET9oPTI3Du"
      },
      "outputs": [],
      "source": [
        "# Generate embeddings\n",
        "embeddings = HuggingFaceEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bJUj5kP9Himg"
      },
      "outputs": [],
      "source": [
        "#@title **creating vector store**\n",
        "\n",
        "# from langchain_community.vectorstores import FAISS\n",
        "# vector_store = FAISS.from_documents(documents, embeddings)\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# creating Chroma vector store\n",
        "vector_store = Chroma.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=\"chroma_db\"  # Optional: persist to disk\n",
        ")\n",
        "\n",
        "# Connect retriever (same interface)\n",
        "default_retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})  # Fetch top 3 chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "l_2H3GeiHvtP"
      },
      "outputs": [],
      "source": [
        "#@title **Craft the prompt template**\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "prompt = \"\"\"\n",
        "**Rules**:\n",
        "1. You are a flipkart customer service agent.\n",
        "2. Answer only based on the context.\n",
        "3. If context doesn't contain answer, list common reasons.\n",
        "3. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "4. Keep answers under 4 sentences.\n",
        "5. you can greet to user.\n",
        "6. Format the answer in Markdown.  Include headings, lists, code blocks, and other Markdown elements as appropriate.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZC1JxqbDKH3W"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AunWTWh9YSvI"
      },
      "outputs": [],
      "source": [
        "#@title **LLM Max input token truncation**\n",
        "\n",
        "from langchain.schema import BaseRetriever, Document\n",
        "from pydantic import Field\n",
        "from typing import List\n",
        "\n",
        "class TokenSafeRetriever(BaseRetriever):\n",
        "    vector_store: object = Field(...)  # Proper Pydantic field declaration\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True  # Allow custom vector store type\n",
        "\n",
        "    def _get_relevant_documents(self, query: str, **kwargs) -> List[Document]:\n",
        "        docs = self.vector_store.similarity_search(query, k=3)\n",
        "        encoder = get_encoding(\"cl100k_base\")\n",
        "\n",
        "        # token truncation logic\n",
        "        max_tokens = 3800 # @param {\"type\":\"number\",\"placeholder\":\"3800\"}\n",
        "        current_tokens = 0\n",
        "        filtered_docs = []\n",
        "\n",
        "        for doc in docs:\n",
        "            doc_tokens = len(encoder.encode(doc.page_content))\n",
        "            if current_tokens + doc_tokens <= max_tokens:\n",
        "                filtered_docs.append(doc)\n",
        "                current_tokens += doc_tokens\n",
        "            else:\n",
        "                remaining = max_tokens - current_tokens\n",
        "                truncated = \" \".join(doc.page_content.split()[:int(remaining*0.7)])\n",
        "                filtered_docs.append(Document(\n",
        "                    page_content=truncated,\n",
        "                    metadata=doc.metadata\n",
        "                ))\n",
        "                break\n",
        "\n",
        "        return filtered_docs\n",
        "\n",
        "# Initialize with proper Pydantic\n",
        "custom_retriever = TokenSafeRetriever(vector_store=vector_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "K4-r9H7yH6LT"
      },
      "outputs": [],
      "source": [
        "#@title **RAG pipeline**\n",
        "from langchain.chains import LLMChain, StuffDocumentsChain\n",
        "# Chain 1: Generate answers\n",
        "llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)\n",
        "\n",
        "# Chain 2: Combine document chunks\n",
        "document_prompt = PromptTemplate(\n",
        "    template=\"Context:\\ncontent:{page_content}\\nsource:{source}\",\n",
        "    input_variables=[\"page_content\", \"source\"]\n",
        ")\n",
        "\n",
        "# Final RAG pipeline\n",
        "qa = RetrievalQA(\n",
        "    combine_documents_chain=StuffDocumentsChain(\n",
        "        llm_chain=llm_chain,\n",
        "        document_prompt=document_prompt,\n",
        "         document_variable_name=\"context\"\n",
        "    ),\n",
        "    return_source_documents=True,\n",
        "retriever = custom_retriever # @param [\"custom_retriever\",\"default_retriever\"] {\"type\":\"raw\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tf_7Ix3SUho3",
        "outputId": "706ba1e5-c069-4633-bf3e-a9396029f2a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "#@title **Customer Sentiment Analysis**\n",
        "from transformers import pipeline\n",
        "# 1. Sentiment Analysis Model\n",
        "sentiment_analyzer = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        ")\n",
        "\n",
        "# 2. Escalation Check Function\n",
        "def requires_human_escalation(query):\n",
        "    # Check for explicit requests\n",
        "    explicit_triggers = {\n",
        "        \"human agent\", \"talk to manager\", \"supervisor\",\n",
        "        \"real person\", \"angry\", \"frustrated\"\n",
        "    }\n",
        "\n",
        "    if any(trigger in query.lower() for trigger in explicit_triggers):\n",
        "        print('explicit_triggers')\n",
        "        return True\n",
        "\n",
        "    # Analyze sentiment\n",
        "    result = sentiment_analyzer(query)[0]\n",
        "    if result['label'] in ['1 star', '2 stars']:  # Negative sentiment\n",
        "        print(result['score'])\n",
        "        return result['score'] > 0.85  # High confidence\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "zNAYxHFuIKJF",
        "outputId": "b71046d8-0477-4e1a-8393-c9dfb738e43c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your query: Why was my payment declined?\n",
            "0.5234935879707336\n",
            "\n",
            "Bot: I don't have specific information on why your payment was declined. Here are some common reasons:\n",
            "\n",
            "- **Insufficient Funds**: Your account may not have enough balance.\n",
            "- **Incorrect Details**: The payment information provided might be incorrect.\n",
            "- **Card Expiry**: Your card might have expired.\n",
            "- **Security Concerns**: There might be a security issue with your card or account.\n",
            "- **Bank Restrictions**: Your bank might have restrictions on online transactions.\n",
            "\n",
            "Sources: ['knowledge_base/terms.pdf']\n",
            "Enter your query: show me cute cat videos\n",
            "\n",
            "Bot: I'm sorry, but I don't have the capability to show videos. However, you can find cute cat videos on various platforms like YouTube, TikTok, or Instagram. Here are some common reasons why you might not be able to find them directly on Flipkart:\n",
            "\n",
            "1. **Platform Specialization**: Flipkart specializes in e-commerce and does not host video content.\n",
            "2. **Content Availability**: Video content, especially entertainment videos, is typically hosted on platforms designed for media consumption.\n",
            "3. **Search Functionality**: Flipkart's search function is optimized for products and services, not media content.\n",
            "\n",
            "Sources: ['knowledge_base/sitemap.pdf', 'knowledge_base/sitemap.md']\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-9e748cc19fa1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your query: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'exit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Retrieve and generate response using \"query\" as the input key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    query = input(\"Enter your query: \")\n",
        "    if query.lower() in ['exit', 'quit',':q']:\n",
        "        break\n",
        "    # Retrieve and generate response using \"query\" as the input key\n",
        "    if requires_human_escalation(query):\n",
        "            print(\"\\nBot: I'm truly sorry you're having this experience. \")\n",
        "            print(\"Let me connect you with a senior support agent immediately.\")\n",
        "            print(\"Please hold while I transfer your chat...\")\n",
        "            # integration with live agent system\n",
        "            break\n",
        "\n",
        "    result = qa({\"query\": query})\n",
        "    full_response = result[\"result\"]\n",
        "\n",
        "    # Extract only the part after \"Answer:\" if present\n",
        "    if \"Answer:\" in full_response:\n",
        "        answer = full_response.split(\"Answer:\")[-1].strip()\n",
        "    else:\n",
        "        answer = full_response.strip()\n",
        "\n",
        "    sources = list(set([doc.metadata[\"source\"] for doc in result[\"source_documents\"]]))\n",
        "\n",
        "    print(f\"\\nBot: {answer}\")\n",
        "    print(f\"\\nSources: {sources}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlta2j-VtQbt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vaimoKrU4kG7",
        "ZazokNW9FNI-"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}