{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sibikrish3000/chatbot_workshop/blob/main/RAG_with_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is RAG?**\n",
        "- **Definition**: A hybrid approach combining retrieval-based and generative models.\n",
        "- **Purpose**: Enhances chatbots by retrieving relevant documents from a knowledge base and generating context-aware responses.\n",
        "\n",
        "### **Key Components**:\n",
        "1. **Retrieval**:\n",
        "   - Searches a database/documents (e.g., Wikipedia, company files) for chunks relevant to the user’s query.\n",
        "   - Uses semantic search (e.g., cosine similarity with embeddings) to find contextually similar text..\n",
        "2. **Augmentation**:\n",
        "  - Injects the retrieved information into the prompt as context for the generative model.\n",
        "3. **Generation**:\n",
        "   - Employs a language model (e.g., DeepSeek, Qwen) to generate natural language responses based on retrieved context.\n",
        "\n",
        "### **Workflow**:\n",
        "1. **Query Input**: User submits a question.\n",
        "2. **Document Retrieval**: Relevant documents are fetched from the knowledge base.\n",
        "3. **Prompt Construction**: Context and query are combined into a prompt.\n",
        "4. **Response Generation**: The model generates a response using the context.\n",
        "5. **Output**: Cleaned and formatted response is returned to the user.\n",
        "\n",
        "### **Benefits**:\n",
        "- Accurate and context-aware responses.\n",
        "- Scalable with large knowledge bases.\n",
        "- Improves conversational flow and user satisfaction."
      ],
      "metadata": {
        "id": "wB5b4hm-KEaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Advavance Rag Implemenation Flow chart:**[![3JB0VdF.md.png](https://iili.io/3JB0VdF.png)](https://freeimage.host/i/3JB0VdF)"
      ],
      "metadata": {
        "id": "2k6WRFykKV-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Naive RAG:**\n",
        "![](https://iili.io/3JRglls.png)\n",
        "### **Workflow in RAG**\n",
        "1. **Embedding Conversion**:  \n",
        "   - The query and all documents are converted into dense vector representations via an embedding model.\n",
        "2. **Similarity Calculation**:  \n",
        "   - Cosine similarity is computed between the query vector and every document vector.\n",
        "3. **Top-k Retrieval**:  \n",
        "   - Documents with the highest cosine scores are selected as context for the generative model (e.g., GPT-3).\n",
        "4. **Generation**:  \n",
        "   - The retrieved documents inform the generative model to produce a relevant, accurate answer.\n",
        "![](https://iili.io/3JYWszg.png)"
      ],
      "metadata": {
        "id": "Qd-fvTJTvDTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Documents\n",
        "question = \"What kinds of pets do I like?\" # @param [\"What kinds of pets do I like?\"] {\"allow-input\":true}\n",
        "document = \"My favorite pet is a cat.\"    # @param [\"My favorite pet is a cat.\"] {\"allow-input\":true}"
      ],
      "metadata": {
        "id": "74fYGlZ1K8ty"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 : Indexing\n",
        "![](https://iili.io/3JBjI1f.png)"
      ],
      "metadata": {
        "id": "xjpEUdIoLmpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "9NYLGc1-L0f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What are tokens?\n",
        ">Tokens can be thought of as pieces of words. Before the API processes the request, the input is broken down into tokens. These tokens are not cut up exactly where the words start or end - tokens can include trailing spaces and even sub-words. Here are some helpful rules of thumb for understanding tokens in terms of lengths:\n",
        "\n",
        "1 token ~= 4 chars in English\n",
        "\n",
        "1 token ~= ¾ words\n",
        "\n",
        "100 tokens ~= 75 words\n",
        "\n",
        "Or\n",
        "\n",
        "1-2 sentence ~= 30 tokens\n",
        "\n",
        "1 paragraph ~= 100 tokens\n",
        "\n",
        "1,500 words ~= 2048 tokens"
      ],
      "metadata": {
        "id": "dICcW3tBP4Bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    enc_str=encoding.encode(string)\n",
        "    print(f'encoded string: {enc_str}')\n",
        "    dec_str=encoding.decode(enc_str)\n",
        "    print(f'Decode string: {dec_str}')\n",
        "    num_tokens = len(enc_str)\n",
        "    return num_tokens\n",
        "\n",
        "num_tokens_from_string(question, \"cl100k_base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz5L6YrNLJnf",
        "outputId": "bd1fb4c4-716f-4c11-d93e-14171adbf9ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded string: [72, 3021, 11058]\n",
            "Decode string: i love coding\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2 : Embedding\n",
        ">embedding is a representation of data (such as words, sentences, or other entities) in a continuous vector space. These vectors are typically dense, meaning they consist of real numbers rather than sparse binary values, and are designed to capture semantic relationships between the items being represented.\n",
        "\n",
        "Key Concepts of Embeddings:\n",
        "\n",
        "Vector Representation :\n",
        "\n",
        "Embeddings map discrete objects (e.g., words, images, or categories) into continuous vector spaces. For example, a word like \"king\" might be represented as a vector [0.25, -0.1, 0.9, ...] in a high-dimensional space.\n",
        "\n",
        "----\n",
        "\n",
        "Semantic Meaning :\n",
        "\n",
        "The key idea behind embeddings is that similar items will have similar vector representations. For instance, in word embeddings, semantically related words (like \"king\" and \"queen\") will have vectors that are close to each other in the vector space.\n",
        "\n",
        "* Suppose you have the words \"king\", \"queen\", \"man\", and \"woman\". After training, their embeddings might look something like this in a simplified 2D space:\n",
        "```\n",
        "king   -> [0.8, 0.2]\n",
        "queen  -> [0.7, 0.3]\n",
        "man    -> [0.9, 0.1]\n",
        "woman  -> [0.8, 0.4]\n",
        "```\n",
        "In this case, the vector for \"king\" is closer to \"man\" than to \"woman\", and \"queen\" is closer to \"woman\". Additionally, the relationship between \"king\" and \"queen\" might be similar to the relationship between \"man\" and \"woman\", which can be captured by vector arithmetic:\n",
        "```\n",
        "king - man + woman ≈ queen\n",
        "```"
      ],
      "metadata": {
        "id": "om-JWDfSMPuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "XhHC3Ub_NfuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "embd = HuggingFaceEmbeddings()\n",
        "query_result = embd.embed_query(question)\n",
        "document_result = embd.embed_query(document)\n",
        "# print(f'query_result: {query_result}')\n",
        "print(f'query_result length: {len(query_result)}')\n",
        "# print(f'document_result: {document_result}')\n",
        "print(f'document_result length: {len(document_result)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1gN9KhDMUy8",
        "outputId": "a470a78c-810e-4fa5-f33d-d4c574e561bc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-26fad8db3a35>:2: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  embd = HuggingFaceEmbeddings()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_result length: 768\n",
            "document_result length: 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part3 : Cosine similarity\n",
        "Cosine similarity is a mathematical measure used to determine how similar two vectors are, regardless of their magnitude. In the context of Retrieval-Augmented Generation (RAG) , cosine similarity plays a crucial role in retrieving relevant information from a knowledge base or corpus before generating responses.\n",
        "\n",
        "---\n",
        "\n",
        "- **Mathematical Formula**:  \n",
        "   The cosine similarity between two vectors $ A $ and $ B $ is given by:\n",
        "   $$\n",
        "   \\text{Cosine Similarity}(A, B) = \\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}\n",
        "   $$\n",
        "   Where:\n",
        "   - $ A \\cdot B $ is the dot product of $ A $ and $ B $: $ A \\cdot B = \\sum_{i=1}^n A_i B_i $\n",
        "   - $ \\|A\\| $ and $ \\|B\\| $ are the magnitudes (or Euclidean norms) of $ A $ and $ B $, respectively:\n",
        "\n",
        "    $$\n",
        "    \\|A\\| = \\sqrt{\\sum_{i=1}^n A_i^2}, \\quad \\|B\\| = \\sqrt{\\sum_{i=1}^n B_i^2} $$\n",
        "---\n",
        "### **Key Properties**\n",
        "1. **Range**:  \n",
        "   - Ranges from **-1 to 1** (but **0 to 1** for non-negative vectors, e.g., TF-IDF document vectors).  \n",
        "   - **1**: Vectors are identical in direction.  \n",
        "   - **0**: Vectors are orthogonal (no similarity).  \n",
        "   - **-1**: Vectors are diametrically opposed.\n",
        "\n",
        "2. **Magnitude Invariance**:  \n",
        "   - Focuses on orientation, not magnitude. Useful when comparing objects where size differences are irrelevant (e.g., text documents of varying lengths).\n",
        "---\n",
        "### **Example Calculation**\n",
        "For vectors $ \\mathbf{A} = [1, 2] $ and $ \\mathbf{B} = [2, 1] :$  \n",
        "1. **Dot Product**:$ (1 \\times 2) + (2 \\times 1) = 4 . $\n",
        "2. **Magnitudes**: $ \\|\\mathbf{A}\\| = \\sqrt{1^2 + 2^2} = \\sqrt{5} ,  \\|\\mathbf{B}\\| = \\sqrt{2^2 + 1^2} = \\sqrt{5} .  $\n",
        "3. **Cosine Similarity**: $ \\frac{4}{\\sqrt{5} \\times \\sqrt{5}} = \\frac{4}{5} = 0.8 . $\n"
      ],
      "metadata": {
        "id": "_bMRjEZKOoQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "similarity = cosine_similarity(query_result, document_result)\n",
        "print(\"Cosine Similarity:\", similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtDxn90UOiKC",
        "outputId": "df08c378-a4fb-43f7-8e9a-62e44570e159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5 : Loader"
      ],
      "metadata": {
        "id": "tvXOMrAnRk-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### INDEXING ####\n",
        "\n",
        "# Loading blog\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "blog_docs = loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkamzqkaRoB-",
        "outputId": "b984dce5-14f8-4477-b2e9-c8edceeaec1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blog_docs"
      ],
      "metadata": {
        "id": "-vpND8lFRwyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6 : [**Text Splitter**](https://python.langchain.com/docs/how_to/recursive_text_splitter/)\n",
        "\n",
        "\n",
        "\n",
        "> This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f_KKFRMqSUZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50)\n",
        "\n",
        "# Make splits\n",
        "splits = text_splitter.split_documents(blog_docs)"
      ],
      "metadata": {
        "id": "dzYLo5lZTF8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaTBNBoKTHog",
        "outputId": "b8ec7348-16b9-4652-a1f9-339eaf54c596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 7 : Vectorstore"
      ],
      "metadata": {
        "id": "cyZsjJZTTzeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "02kcJJg_UHm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "vectorstore = Chroma.from_documents(documents=splits,\n",
        "                                    embedding=HuggingFaceEmbeddings())"
      ],
      "metadata": {
        "id": "OFUXT1DIT_E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "id": "-BbrPqEzU5S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")"
      ],
      "metadata": {
        "id": "mF2mOkxOUtjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9TV7f_CVXUc",
        "outputId": "0fab5088-fab1-4ea5-a9e9-e8289a9bc60b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKFTF9dIUu1Y",
        "outputId": "72c38332-1125-47e9-8ad3-c580ca6be63d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:')"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3THSM2k2VKgV",
        "outputId": "d2e73da5-db8f-4c38-f030-04de1bbf0183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 8: Generation\n",
        "![](https://iili.io/3JCzBQn.png)"
      ],
      "metadata": {
        "id": "VzpnIV5fVuuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Prompt\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCeK_r3qV1RC",
        "outputId": "d5857d00-1f39-418b-ca83-2981760f845e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-huggingface"
      ],
      "metadata": {
        "id": "-o77zrMeXG0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "# LLM\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"deepseek-ai/DeepSeek-R1\",\n",
        "     temperature=0.2 #@param {\"type\":\"number\",\"placeholder\":\"0.7\"}\n",
        "    ,\n",
        "    max_length=512,\n",
        "    task='text-generation',\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBOMO8D5Wdlw",
        "outputId": "490836dc-316b-4439-b4a3-6ef5e55ac386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_huggingface.llms.huggingface_endpoint:WARNING! max_length is not default parameter.\n",
            "                    max_length was transferred to model_kwargs.\n",
            "                    Please make sure that max_length is what you intended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain\n",
        "\n",
        "chain = prompt | llm #chain pipeline\n",
        "\n",
        "# ### without pipeline\n",
        "# input=prompt.format_prompt(context=docs[0].page_content, question=\"What is Task Decomposition?\")\n",
        "# response=llm.invoke(input)\n",
        "# response"
      ],
      "metadata": {
        "id": "uWoIGcVhXxGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "chain.invoke({\"context\":lambda docs: \"\\n\\n\".join(d.page_content for d in docs|),\n",
        "                                                 \"question\":\"What is Task Decomposition?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "uirDvx7WZ4gd",
        "outputId": "278815c0-313d-43b5-fc38-d336ec38e1db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer: Task Decomposition is the process of breaking down a complex task into smaller, more manageable steps or subgoals. This can be done by a Large Language Model (LLM) using simple prompts, task-specific instructions, or with human inputs.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\") #@markdown create a langsmith acount here [**smith.langchain.com**](https://smith.langchain.com/) then generate langsmith API key"
      ],
      "metadata": {
        "id": "wGABtIoNankX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_hub_rag"
      ],
      "metadata": {
        "id": "du0xRVHIb7c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [**RAG chains**]()"
      ],
      "metadata": {
        "id": "JpD3Wq84cAry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": retriever | (lambda docs: \"\\n\\n\".join(d.page_content for d in docs)),\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    | prompt_hub_rag\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "qiAcXfr0clNf",
        "outputId": "5870300d-64fd-475c-f025-cc6a5731ff7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer: Task Decomposition is the process of breaking down a complex task into smaller, more manageable subtasks. This can be done by the LLM with simple prompting, using task-specific instructions, or with human inputs. The goal is to transform big tasks into multiple manageable tasks to make them easier to understand and execute.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaimoKrU4kG7"
      },
      "source": [
        "# Installing Required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjhXLrM5D2tP"
      },
      "outputs": [],
      "source": [
        "#@title <b>Installing Required packages</b>\n",
        "!pip install langchain faiss-cpu sentence-transformers transformers accelerate pypdf langchain-experimental langchain-groq langchain-community unstructured[all]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yObw6VkH4qaX"
      },
      "source": [
        "# RAG with API's"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB26SnEAEcgP"
      },
      "source": [
        "### Groq API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J3O3n6ceER9o"
      },
      "outputs": [],
      "source": [
        "#@title <b>RAG implementaion with Groq API </b>\n",
        "import dotenv\n",
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "groq_api_key = userdata.get('GROQ_API_KEY') # @param [\"userdata.get('GROQ_API_KEY')\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "\n",
        "# ## load the Groq API key\n",
        "# dotenv.load_dotenv()\n",
        "# groq_api_key=os.environ['GROQ_API_KEY']\n",
        "\n",
        "model_name = \"llama-3.2-3b-preview\" # @param [\"llama-3.2-3b-preview\",\"deepseek-r1-distill-llama-70b\",\"llama-3.3-70b-versatile\",\"llama-3.3-70b-specdec\",\"llama-3.2-1b-preview\",\"llama-3.1-8b-instant\",\"llama3-70b-8192\",\"mixtral-8x7b-32768\",\"llama3-8b-8192\",\"llama-guard-3-8b\"]\n",
        "\n",
        "llm=ChatGroq(groq_api_key=groq_api_key,\n",
        "             model_name=model_name,\n",
        "temperature = 0.7 # @param {\"type\":\"number\",\"placeholder\":\"0.7\"}\n",
        "             )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEOheMm8Em7v"
      },
      "source": [
        "### Mistralai API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oOYv6wOofNTw"
      },
      "outputs": [],
      "source": [
        "#@title <b>RAG implementaion with Mistral API\n",
        "from google.colab import userdata\n",
        "\n",
        "mistral_api_key = userdata.get('MISTRAL_API_KEY') # @param [\"userdata.get('MISTRAL_API_KEY')\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "\n",
        "model_name = \"mistral-small-latest\" # @param [\"mistral-small-latest\",\"pixtral-large-latest\",\"codestral-latest\",\"mistral-large-latest\",\"ministral-8b-latest\",\"ministral-3b-latest\",\"open-mistral-nemo\"]\n",
        "\n",
        "llm = ChatMistralAI(\n",
        "    mistral_api_key=mistral_api_key,\n",
        "    model=model_name,\n",
        "    temperature = 0.7 # @param {\"type\":\"number\",\"placeholder\":\"0.7\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iE8EvQvaOOT5"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-mistralai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ObpLsQnE5C0"
      },
      "source": [
        "### Google GenerativeAI API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ngc4-nXhjZhX"
      },
      "outputs": [],
      "source": [
        "#@title <b>RAG implementaion with Google GenerativeAI API\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# 1. Configure Google AI Studio get from https://makersuite.google.com/\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY') # @param [\"userdata.get('GOOGLE_API_KEY')\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "\n",
        "model_name = \"gemini-2.0-flash\" # @param [\"gemini-2.0-flash\",\"gemini-2.0-flash-lite-preview-02-05\",\"gemini-2.0-pro-exp-02-05\",\"gemini-2.0-flash-thinking-exp-01-21\",\"gemini-2.0-flash-exp\",\"learnlm-1.5-pro-experimental\",\"gemini-1.5-pro\",\"gemini-1.5-flash\",\"gemini-1.5-8b\",\"gemini-1.5-flash-8b\"]\n",
        "\n",
        "# 2. Initialize Gemini model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY,\n",
        "    temperature = 0.7 # @param {\"type\":\"number\",\"placeholder\":\"0.7\"}\n",
        "    ,\n",
        "    convert_system_message_to_human=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l44-V9pdOWN3"
      },
      "outputs": [],
      "source": [
        "!pip install google-generativeai langchain-google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Running RAG with Locally with Ollama**\n"
      ],
      "metadata": {
        "id": "KAOyLHy70w2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Downloading Ollama**\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "_cPg1-hi1FNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash --bg\n",
        "#@title Starting ollama server in colab background\n",
        "ollama serve > /dev/null 2>&1  # Start server in background, suppress logs"
      ],
      "metadata": {
        "id": "pTeac1ZA1lK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl http://localhost:11434  # Should return \"Ollama is running\""
      ],
      "metadata": {
        "id": "wDrKy5G93XCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Downloading Deepseek-r1:7.5b Locally\n",
        "!ollama pull deepseek-r1"
      ],
      "metadata": {
        "id": "6W2T70252sA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#List the local Ollama models\n",
        "!ollama list"
      ],
      "metadata": {
        "id": "JVL7z5723AkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **kill Running Ollama**\n",
        "!pkill -f \"ollama serve\" #Dont run this cell unless u want to stop ollama server"
      ],
      "metadata": {
        "id": "N0ZyBvED4EyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Testing the Deepseek-r1  model in locally\n",
        "!ollama run --verbose deepseek-r1 'hi'"
      ],
      "metadata": {
        "id": "ENjMeIto3nih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U langchain-ollama"
      ],
      "metadata": {
        "id": "CyXg_YJF6eC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Initialize the Ollama model with Python Langchain**\n",
        "\n",
        "from langchain_community.llms import Ollama\n",
        "# from langchain_ollama.llms import OllamaLLM\n",
        "\n",
        "# Initialize the Ollama model\n",
        "llm = Ollama(\n",
        "    model=\"deepseek-r1\",\n",
        "    temperature=0.9,\n",
        "    base_url='http://localhost:11434'\n",
        ")\n",
        "\n",
        "# Run inference\n",
        "response = llm.invoke(\"Explain quantum computing in 3 sentences.\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "RkARqFwP5-Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZazokNW9FNI-"
      },
      "source": [
        "### Testing LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX3SQP78En4T",
        "outputId": "f0379f83-88c2-4b09-a03a-c47830f05ead"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Medicaid managed care is a system where a state contracts with managed care organizations (MCOs) to administer healthcare services to Medicaid beneficiaries. Instead of the state directly paying providers for each service (fee-for-service), the state pays the MCO a fixed per-member, per-month (capitation) rate to cover the cost of healthcare for the enrollees in that plan.\\n\\nHere's a breakdown of key aspects:\\n\\n*   **How it works:** States contract with MCOs (which can be HMOs, provider-sponsored organizations, or other types of health plans) to provide a defined set of healthcare services to Medicaid enrollees. Enrollees typically choose a managed care plan from a selection offered in their area.\\n\\n*   **Capitation:** The MCO receives a fixed payment per member per month, regardless of how much or how little healthcare the enrollee uses. This incentivizes the MCO to manage costs and promote preventive care.\\n\\n*   **Network of Providers:** MCOs create and manage a network of doctors, hospitals, and other healthcare providers that enrollees can access. Enrollees typically need to use providers within the plan's network to have their care covered.\\n\\n*   **Goals:** States use managed care to:\\n    *   Control Medicaid costs\\n    *   Improve access to care\\n    *   Enhance the quality of care\\n    *   Promote care coordination\\n\\n*   **Types of Managed Care:** There are different models, including:\\n    *   **Comprehensive Risk-Based Managed Care:** MCOs assume full financial risk for a comprehensive set of services.\\n    *   **Primary Care Case Management (PCCM):** States contract with primary care providers to manage the care of Medicaid enrollees.\\n    *   **Limited Benefit Plans:** MCOs provide a limited set of services, such as behavioral health or dental care.\\n\\n*   **Enrollment:** In many states, Medicaid beneficiaries are required to enroll in a managed care plan. Some populations, such as individuals with disabilities or those requiring long-term care, may be excluded or have specialized managed care options.\\n\\n*   **Oversight:** States and the federal government (Centers for Medicare & Medicaid Services - CMS) oversee Medicaid managed care plans to ensure they meet quality standards, provide adequate access to care, and comply with regulations.\\n\\nIn summary, Medicaid managed care is a significant way that states deliver healthcare services to Medicaid beneficiaries, aiming to improve efficiency, access, and quality of care through contracts with managed care organizations.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-82bbeee1-b1f2-4b46-9b2e-e96fee2acc02-0', usage_metadata={'input_tokens': 24, 'output_tokens': 526, 'total_tokens': 550, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.schema.messages import HumanMessage, SystemMessage\n",
        "messages = [\n",
        " SystemMessage(\n",
        "      content=\"\"\"You're an assistant knowledgeable about\n",
        "     healthcare. Only answer healthcare-related questions.\"\"\"\n",
        "),\n",
        " HumanMessage(content=\"What is Medicaid managed care?\"),\n",
        "]\n",
        "llm.invoke(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q5cCtLqFvUw"
      },
      "source": [
        "# Building a RAG Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LGZFw9plFUqj"
      },
      "outputs": [],
      "source": [
        "#@title <b>Load documents with multiple file support</b>\n",
        "from langchain.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader, UnstructuredMarkdownLoader\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "import os\n",
        "# 1. Load documents with multiple file support\n",
        "def load_documents(path):\n",
        "    loaders = [\n",
        "        DirectoryLoader(path, glob=\"**/*.pdf\", loader_cls=PyPDFLoader),\n",
        "        DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader),\n",
        "        DirectoryLoader(path, glob=\"**/*.md\", loader_cls=UnstructuredMarkdownLoader)\n",
        "    ]\n",
        "    documents = []\n",
        "    for loader in loaders:\n",
        "        documents.extend(loader.load())\n",
        "    return documents\n",
        "\n",
        "knowledge_base_path = \"knowledge_base/\" # @param [\"knowledge_base/\"] {\"allow-input\":true}\n",
        "docs = load_documents(knowledge_base_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFgmxZOfGrQK"
      },
      "outputs": [],
      "source": [
        "#@title <b>Splitting text into semantic chunks</b>\n",
        "text_splitter = SemanticChunker(HuggingFaceEmbeddings())\n",
        "documents = text_splitter.split_documents(docs)\n",
        "##cell completed with cpu 12min 23sec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0RKCM73L98U"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1ET9oPTI3Du"
      },
      "outputs": [],
      "source": [
        "# Generate embeddings\n",
        "embeddings = HuggingFaceEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bJUj5kP9Himg"
      },
      "outputs": [],
      "source": [
        "#@title **creating vector store**\n",
        "\n",
        "# from langchain_community.vectorstores import FAISS\n",
        "# vector_store = FAISS.from_documents(documents, embeddings)\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# creating Chroma vector store\n",
        "vector_store = Chroma.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=\"chroma_db\"  # Optional: persist to disk\n",
        ")\n",
        "\n",
        "# Connect retriever (same interface)\n",
        "default_retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})  # Fetch top 3 chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "l_2H3GeiHvtP"
      },
      "outputs": [],
      "source": [
        "#@title **Craft the prompt template**\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "prompt = \"\"\"\n",
        "**Rules**:\n",
        "1. You are a flipkart customer service agent.\n",
        "2. Answer only based on the context.\n",
        "3. If context doesn't contain answer, list common reasons.\n",
        "3. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "4. Keep answers under 4 sentences.\n",
        "5. you can greet to user.\n",
        "6. Format the answer in Markdown.  Include headings, lists, code blocks, and other Markdown elements as appropriate.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZC1JxqbDKH3W"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AunWTWh9YSvI"
      },
      "outputs": [],
      "source": [
        "#@title **LLM Max input token truncation**\n",
        "\n",
        "from langchain.schema import BaseRetriever, Document\n",
        "from pydantic import Field\n",
        "from typing import List\n",
        "\n",
        "class TokenSafeRetriever(BaseRetriever):\n",
        "    vector_store: object = Field(...)  # Proper Pydantic field declaration\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True  # Allow custom vector store type\n",
        "\n",
        "    def _get_relevant_documents(self, query: str, **kwargs) -> List[Document]:\n",
        "        docs = self.vector_store.similarity_search(query, k=3)\n",
        "        encoder = get_encoding(\"cl100k_base\")\n",
        "\n",
        "        # token truncation logic\n",
        "        max_tokens = 3800 # @param {\"type\":\"number\",\"placeholder\":\"3800\"}\n",
        "        current_tokens = 0\n",
        "        filtered_docs = []\n",
        "\n",
        "        for doc in docs:\n",
        "            doc_tokens = len(encoder.encode(doc.page_content))\n",
        "            if current_tokens + doc_tokens <= max_tokens:\n",
        "                filtered_docs.append(doc)\n",
        "                current_tokens += doc_tokens\n",
        "            else:\n",
        "                remaining = max_tokens - current_tokens\n",
        "                truncated = \" \".join(doc.page_content.split()[:int(remaining*0.7)])\n",
        "                filtered_docs.append(Document(\n",
        "                    page_content=truncated,\n",
        "                    metadata=doc.metadata\n",
        "                ))\n",
        "                break\n",
        "\n",
        "        return filtered_docs\n",
        "\n",
        "# Initialize with proper Pydantic\n",
        "custom_retriever = TokenSafeRetriever(vector_store=vector_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4-r9H7yH6LT"
      },
      "outputs": [],
      "source": [
        "#@title **RAG pipeline**\n",
        "from langchain.chains import LLMChain, StuffDocumentsChain\n",
        "# Chain 1: Generate answers\n",
        "llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)\n",
        "\n",
        "# Chain 2: Combine document chunks\n",
        "document_prompt = PromptTemplate(\n",
        "    template=\"Context:\\ncontent:{page_content}\\nsource:{source}\",\n",
        "    input_variables=[\"page_content\", \"source\"]\n",
        ")\n",
        "\n",
        "# Final RAG pipeline\n",
        "qa = RetrievalQA(\n",
        "    combine_documents_chain=StuffDocumentsChain(\n",
        "        llm_chain=llm_chain,\n",
        "        document_prompt=document_prompt,\n",
        "         document_variable_name=\"context\"\n",
        "    ),\n",
        "    return_source_documents=True,\n",
        "retriever = default_retriever # @param [\"custom_retriever\",\"default_retriever\"] {\"type\":\"raw\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tf_7Ix3SUho3"
      },
      "outputs": [],
      "source": [
        "#@title **Customer Sentiment Analysis**\n",
        "from transformers import pipeline\n",
        "# 1. Sentiment Analysis Model\n",
        "sentiment_analyzer = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        ")\n",
        "\n",
        "# 2. Escalation Check Function\n",
        "def requires_human_escalation(query):\n",
        "    # Check for explicit requests\n",
        "    explicit_triggers = {\n",
        "        \"human agent\", \"talk to manager\", \"supervisor\",\n",
        "        \"real person\", \"angry\", \"frustrated\"\n",
        "    }\n",
        "\n",
        "    if any(trigger in query.lower() for trigger in explicit_triggers):\n",
        "        print('explicit_triggers')\n",
        "        return True\n",
        "\n",
        "    # Analyze sentiment\n",
        "    result = sentiment_analyzer(query)[0]\n",
        "    if result['label'] in ['1 star', '2 stars']:  # Negative sentiment\n",
        "        print(result['score'])\n",
        "        return result['score'] > 0.85  # High confidence\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNAYxHFuIKJF"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "    query = input(\"Enter your query: \")\n",
        "    if query.lower() in ['exit', 'quit',':q']:\n",
        "        break\n",
        "    # Retrieve and generate response using \"query\" as the input key\n",
        "    if requires_human_escalation(query):\n",
        "            print(\"\\nBot: I'm truly sorry you're having this experience. \")\n",
        "            print(\"Let me connect you with a senior support agent immediately.\")\n",
        "            print(\"Please hold while I transfer your chat...\")\n",
        "            # integration with live agent system\n",
        "            break\n",
        "\n",
        "    result = qa({\"query\": query})\n",
        "    full_response = result[\"result\"]\n",
        "\n",
        "    # Extract only the part after \"Answer:\" if present\n",
        "    if \"Answer:\" in full_response:\n",
        "        answer = full_response.split(\"Answer:\")[-1].strip()\n",
        "    else:\n",
        "        answer = full_response.strip()\n",
        "\n",
        "    sources = list(set([doc.metadata[\"source\"] for doc in result[\"source_documents\"]]))\n",
        "\n",
        "    print(f\"\\nBot: {answer}\")\n",
        "    print(f\"\\nSources: {sources}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Integrating RAG Agent into Telegram Bot**"
      ],
      "metadata": {
        "id": "M5-FcfGvpVsl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlta2j-VtQbt"
      },
      "outputs": [],
      "source": [
        "!pip install python-telegram-bot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "id": "xZSJ2yhXijmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from telegram import Update\n",
        "from telegram.ext import Application, MessageHandler, filters, ContextTypes\n",
        "import logging\n",
        "from google.colab import userdata\n",
        "import asyncio\n",
        "import nest_asyncio # To run our bot in jupyter notebook\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    level=logging.INFO\n",
        ")\n",
        "\n",
        "async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    query = update.message.text\n",
        "\n",
        "    # Check for exit commands (optional)\n",
        "    if query.lower() in ['exit', 'quit', ':q']:\n",
        "        await update.message.reply_text(\"Goodbye! 👋\")\n",
        "        return\n",
        "\n",
        "    # Human escalation check\n",
        "    if requires_human_escalation(query):\n",
        "        response = (\n",
        "            \"⚠️ I'm truly sorry you're having this experience.\\n\\n\"\n",
        "            \"Let me connect you with a senior support agent immediately...\\n\"\n",
        "            \"Please hold while I transfer your chat ⌛\"\n",
        "        )\n",
        "        await update.message.reply_text(response)\n",
        "        # Add your live agent integration here\n",
        "        return\n",
        "\n",
        "    # Process query with RAG\n",
        "    try:\n",
        "        result = qa({\"query\": query})\n",
        "        full_response = result[\"result\"]\n",
        "\n",
        "        # Extract answer\n",
        "        if \"Answer:\" in full_response:\n",
        "            answer = full_response.split(\"Answer:\")[-1].strip()\n",
        "        else:\n",
        "            answer = full_response.strip()\n",
        "\n",
        "        # Get unique sources\n",
        "        sources = list(set([doc.metadata[\"source\"] for doc in result[\"source_documents\"]]))\n",
        "\n",
        "        # Format response\n",
        "        response = f\"🤖 {answer}\\n\\n📚 Sources:\\n\" + \"\\n\".join(sources)\n",
        "\n",
        "        await update.message.reply_text(response)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing query: {e}\")\n",
        "        await update.message.reply_text(\"⚠️ Sorry, I encountered an error processing your request. Please try again.\")\n",
        "\n",
        "def main():\n",
        "    # Get Telegram token from environment\n",
        "    token =userdata.get('TELEGRAM_BOT_TOKEN') # os.getenv(\"TELEGRAM_BOT_TOKEN\")\n",
        "\n",
        "    # Create Application\n",
        "    application = Application.builder().token(token).build()\n",
        "\n",
        "    # Add message handler\n",
        "    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))\n",
        "\n",
        "    # Start polling\n",
        "    logging.info(\"Bot is running...\")\n",
        "    application.run_polling()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    loop = asyncio.new_event_loop()\n",
        "    asyncio.set_event_loop(loop)\n",
        "    try:\n",
        "        loop.run_until_complete(main())\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "    finally:\n",
        "        loop.close()"
      ],
      "metadata": {
        "id": "5YXHoRUKUHw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this code only for non-notebook python scripts\n",
        "import logging\n",
        "from telegram import Update\n",
        "from telegram.ext import Application, CommandHandler, MessageHandler, ContextTypes, filters\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Enable logging for debugging\n",
        "logging.basicConfig(\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# /start command handler\n",
        "async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
        "    await update.message.reply_text(\"Hello! I'm your simple Telegram bot. How can I help you today?\")\n",
        "\n",
        "# Echo any text message back to the user\n",
        "async def echo(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
        "    # For now, simply echo the incoming message text.\n",
        "    received_text = update.message.text\n",
        "    await update.message.reply_text(received_text)\n",
        "\n",
        "def main() -> None:\n",
        "    # Get Telegram token from environment\n",
        "    token =os.getenv(\"TELEGRAM_BOT_TOKEN\") # userdata.get('TELEGRAM_BOT_TOKEN')\n",
        "    application = Application.builder().token(token).build()\n",
        "\n",
        "    # Add command handler for /start command.\n",
        "    application.add_handler(CommandHandler(\"start\", start))\n",
        "    # Add message handler for text messages.\n",
        "    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, echo))\n",
        "\n",
        "    # Run the bot until the user presses Ctrl-C\n",
        "    application.run_polling()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "YJIKUPE0UUHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VfD3ywKPg0Jg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KAOyLHy70w2w",
        "ZazokNW9FNI-"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}